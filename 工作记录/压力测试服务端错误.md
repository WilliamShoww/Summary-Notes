#		压力测试中出现500服务端错误

##		背景

项目需要进行阶段性压力测试，所以想模拟真实的正式环境，所以是直接使用域名访问进行压力。整个请求链路大概如下：

![image-20210509122004104](./assets/image-20210509122004104.png)

##		现象

客户端现象：在压测的时候，只要客户端线程数量上去，就会收到很多失败请求，返回响应码：500，响应消息：Internal server error 。

服务端现象：Tomcat服务器没有慢响应接口，资源没有满，没有任何错误日志。

##		排查过程

由于经验不是很足，所以第一时间想到的是可能Tomcat容器撑不住了，导致响应超时；我后来又去查询了一下，超时的响应码是：504，响应消息：time out。

并且之前压测没有因为超时而产生服务端错误的情况。所以暂时没有了思路。

后来冷静下来分析，可能Nginx的问题，于是去查询Nginx的配置，果然发现一些小毛病，Nginx配置了一个限流的配置，并且是根据IP作为Key进行桶令牌限流，瞬间觉得问题得到解决，把配置给去了。

再次压测，依然服务端错误，并且只要线程上去就会报，线程降低一些就不会报。顿时又失去了一些思路。

后来向一个测试的同事求助，还好测试的同事经验比较丰富，那你去看一下Nginx的日志啊，也许能找到线索呢。内心卧槽，自己怎么这么蠢，日志都不知道去看。立马跑去看Nginx的日志，果然有错误报出。错误`Too many open files`。顿时问题才得到彻底解决，找运维把Linux的文件句柄限制给改一下。

##		复盘

为啥之前压测同样的机器没有出现这个问题呢？

后来才发现，自己改了Jmeter的配置，把请求的keeplive模式给去了，导致每次请求都会新建链接，从而突破链接数上限。

排查这个问题的方法：使用脚本在服务器上统计各个状态下的socket数量，脚本如下：

```shell
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
```

##		总结

1. 性能问题排查必须每个环节都需要去考虑是否有卡点；
2. 压力测试需要把一些锁化操作全面放开(特别是使用一个账号进行压测的时候)；
3. 遇到问题，第一反应应该是去看数据，看日志，而不是瞎猜；
4. 压力测试，日志输出最好关闭或者异步化打印(之前也是吃过亏的)；
5. 压力测试检查一下各环节的限流配置，全面关闭限流配置。

